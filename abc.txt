services:
  # REST Catalog
  gravitino:
    build:
      context: .
      dockerfile: ./infrastructure/gravitino/Dockerfile
      args:
        - GRAVITINO_IMAGE_TAG=${GRAVITINO_IMAGE_TAG}
        - HADOOP_AWS_JAR_VERSION=${HADOOP_AWS_JAR_VERSION}
        - AWS_JAVA_SDK_BUNDLE_JAR_VERSION=${AWS_JAVA_SDK_BUNDLE_JAR_VERSION}
        - AWS_SDK_V2_VERSION=${AWS_SDK_V2_VERSION}
        - ICEBERG_VERSION=${ICEBERG_VERSION}
    container_name: gravitino
    ports:
      - "8090:8090"
      - "9001:9001"
    env_file:
      - .env
    volumes:
      - ./infrastructure/common:/tmp/common
      - ./infrastructure/gravitino/conf:/tmp/conf
      - ./infrastructure/gravitino/healthcheck:/tmp/healthcheck
      - gravitino_data:/root/gravitino/data
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8090/api/version"]
      start_period: 30s
      interval: 10s
      timeout: 10s
      retries: 5
    networks:
      - local-data-lakehouse-iceberg
      
  # Metastore DB: PostgreSQL
  postgres:
    platform: linux/amd64
    image: postgres:${POSTGRESQL_IMAGE_TAG}
    container_name: postgres
    ports:
      - "5432:5432"
    env_file:
      - .env
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: catalog_metastore_db
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d catalog_metastore_db"]
      interval: 5s
      timeout: 60s
      retries: 5
      start_period: 20s
    networks:
      - local-data-lakehouse-iceberg

  # Distributed Compute Engine: Apache Spark
  spark-master:
    platform: linux/amd64
    image: spark-image
    build:
      context: .
      dockerfile: ./infrastructure/spark/Dockerfile
      args:
        - PYTHON_VERSION=${PYTHON_VERSION}
        - SPARK_SCALA_VERSION=${SPARK_SCALA_VERSION}
        - ICEBERG_VERSION=${ICEBERG_VERSION}
        - SPARK_VERSION=${SPARK_VERSION}
        - POSTGRESQL_JAR_VERSION=${POSTGRESQL_JAR_VERSION}
        - HADOOP_AWS_JAR_VERSION=${HADOOP_AWS_JAR_VERSION}
        - AWS_JAVA_SDK_BUNDLE_JAR_VERSION=${AWS_JAVA_SDK_BUNDLE_JAR_VERSION}
        - KAFKA_VERSION=${KAFKA_VERSION}
        - SPARK_KAFKA_VERSION=${SPARK_KAFKA_VERSION}
    container_name: spark-master
    ports:
      - "8080:8080"
      - "4040:4040"
      - "7077:7077"
      - "8888:8888"
    env_file:
      - .env
      - ./infrastructure/spark/.env.spark
    volumes:
      - ./infrastructure/common:/tmp/common
      - ./infrastructure/spark/conf:/tmp/conf
      - ./pipelines:/opt/spark/apps/pipelines
      - ./setup:/opt/spark/apps/setup
      - ./infrastructure/spark/logs/:/opt/spark/spark-events
      - ./notebooks:/opt/spark/notebooks
      - ./scripts:/opt/spark/scripts
    depends_on:
      gravitino:
        condition: service_healthy
      minio:
        condition: service_healthy
    entrypoint: ["./init/init.sh", "master"]
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 5s
      timeout: 60s
      retries: 5
      start_period: 20s
    networks:
      - local-data-lakehouse-iceberg

  spark-worker-1:
    platform: linux/amd64
    image: spark-image
    build:
      context: .
      dockerfile: ./infrastructure/spark/Dockerfile
      args:
        - PYTHON_VERSION=${PYTHON_VERSION}
        - SPARK_SCALA_VERSION=${SPARK_SCALA_VERSION}
        - ICEBERG_VERSION=${ICEBERG_VERSION}
        - SPARK_VERSION=${SPARK_VERSION}
        - POSTGRESQL_JAR_VERSION=${POSTGRESQL_JAR_VERSION}
        - HADOOP_AWS_JAR_VERSION=${HADOOP_AWS_JAR_VERSION}
        - AWS_JAVA_SDK_BUNDLE_JAR_VERSION=${AWS_JAVA_SDK_BUNDLE_JAR_VERSION}
        - KAFKA_VERSION=${KAFKA_VERSION}
        - SPARK_KAFKA_VERSION=${SPARK_KAFKA_VERSION}
    container_name: spark-worker-1
    env_file:
      - .env
      - ./infrastructure/spark/.env.spark
    volumes:
      - ./infrastructure/common:/tmp/common
      - ./infrastructure/spark/conf:/tmp/conf
      - ./pipelines:/opt/spark/apps/pipelines
      - ./infrastructure/spark/logs/:/opt/spark/spark-events
      - ./scripts:/opt/spark/scripts
    depends_on:
      spark-master:
        condition: service_healthy
    entrypoint: ['./init/init.sh', 'worker']
    networks:
      - local-data-lakehouse-iceberg

  spark-worker-2:
    platform: linux/amd64
    image: spark-image
    build:
      context: .
      dockerfile: ./infrastructure/spark/Dockerfile
      args:
        - PYTHON_VERSION=${PYTHON_VERSION}
        - SPARK_SCALA_VERSION=${SPARK_SCALA_VERSION}
        - ICEBERG_VERSION=${ICEBERG_VERSION}
        - SPARK_VERSION=${SPARK_VERSION}
        - POSTGRESQL_JAR_VERSION=${POSTGRESQL_JAR_VERSION}
        - HADOOP_AWS_JAR_VERSION=${HADOOP_AWS_JAR_VERSION}
        - AWS_JAVA_SDK_BUNDLE_JAR_VERSION=${AWS_JAVA_SDK_BUNDLE_JAR_VERSION}
        - KAFKA_VERSION=${KAFKA_VERSION}
        - SPARK_KAFKA_VERSION=${SPARK_KAFKA_VERSION}
    container_name: spark-worker-2
    env_file:
      - .env
      - ./infrastructure/spark/.env.spark
    volumes:
      - ./infrastructure/common:/tmp/common
      - ./infrastructure/spark/conf:/tmp/conf
      - ./pipelines:/opt/spark/apps/pipelines
      - ./infrastructure/spark/logs/:/opt/spark/spark-events
      - ./scripts:/opt/spark/scripts
    depends_on:
      spark-master:
        condition: service_healthy
    entrypoint: ['./init/init.sh', 'worker']
    networks:
      - local-data-lakehouse-iceberg

  # Object Storage: MinIO
  minio:
    platform: linux/amd64
    image: minio/minio:${MINIO_IMAGE_TAG}
    container_name: minio
    ports:
      - "9000:9000"
      - "9002:9002"
    env_file:
      - .env
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    volumes:
      - minio_data:/data
    command: server /data --address :9000 --console-address ':9002'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 5s
      timeout: 60s
      retries: 5
      start_period: 20s
    networks:
      - local-data-lakehouse-iceberg
  
  minio-client:
    platform: linux/amd64
    image: "minio/mc:${MINIO_CLIENT_IMAGE_TAG}"
    container_name: minio-client
    env_file:
      - .env
    depends_on:
      minio: 
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set myminio http://minio:9000 minioadmin minioadmin123;
      /usr/bin/mc mb myminio/iceberg --ignore-existing;
      /usr/bin/mc mb myminio/iceberg/warehouse --ignore-existing;
      /usr/bin/mc anonymous set public myminio/iceberg/warehouse;
      "
    networks:
      - local-data-lakehouse-iceberg
    
  trino:
    platform: linux/amd64
    image: trinodb/trino:${TRINO_IMAGE_TAG}
    container_name: trino
    ports:
      - "8088:8080"
    env_file:
      - .env
    volumes:
      - ./infrastructure/trino/etc/catalog:/etc/trino/catalog
    depends_on:
      gravitino:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8088/v1/info" ]
      interval: 5s
      timeout: 60s
      retries: 5
      start_period: 20s
    networks:
      - local-data-lakehouse-iceberg

  # Message Queue: Apache Kafka (Single Node KRaft)
  kafka:
    image: apache/kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      - KAFKA_NODE_ID=1
      - KAFKA_PROCESS_ROLES=controller,broker
      - KAFKA_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CLUSTER_ID=abcdefghijklmnopqrstuv
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - local-data-lakehouse-iceberg
    healthcheck:
      test: ["CMD-SHELL", "/opt/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Monitoring: Kafka Exporter
  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: kafka-exporter
    command:
      - '--kafka.server=kafka:9092'
    ports:
      - "9308:9308"
    networks:
      - local-data-lakehouse-iceberg
    depends_on:
      kafka:
        condition: service_healthy

  # Observability: Prometheus
  prometheus:
    image: prom/prometheus:${PROMETHEUS_IMAGE_TAG}
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./infrastructure/prometheus/conf/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    networks:
      - local-data-lakehouse-iceberg
    healthcheck:
      test: ["CMD", "wget", "-q", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Observability: Grafana
  grafana:
    image: grafana/grafana:${GRAFANA_IMAGE_TAG}
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
    networks:
      - local-data-lakehouse-iceberg
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 3

networks:
  local-data-lakehouse-iceberg:

volumes:
  gravitino_data:
  postgres_data:
  minio_data:
  kafka_data:
  prometheus_data:
  grafana_data:
